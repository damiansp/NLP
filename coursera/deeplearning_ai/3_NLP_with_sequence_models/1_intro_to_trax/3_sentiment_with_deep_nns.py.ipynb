{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1:  Sentiment with Deep Neural Networks\n",
    "\n",
    "Welcome to the first assignment of course 3. In this assignment, you will explore sentiment analysis using deep neural networks. \n",
    "\n",
    "In course 1, you implemented Logistic regression and Naive Bayes for sentiment analysis. However if you were to give your old models an example like:\n",
    "\n",
    "<center> <span style='color:blue'> <b>This movie was almost good.</b> </span> </center>\n",
    "\n",
    "Your model would have predicted a positive sentiment for that review. However, that sentence has a negative sentiment and indicates that the movie was not good. To solve those kinds of misclassifications, you will write a program that uses deep neural networks to identify sentiment in text. By completing this assignment, you will: \n",
    "\n",
    "- Understand how you can build/design a model using layers\n",
    "- Train a model using a training loop\n",
    "- Use a binary cross-entropy loss function\n",
    "- Compute the accuracy of your model\n",
    "- Predict using your own input\n",
    "\n",
    "As you can tell, this model follows a similar structure to the one you previously implemented in the second course of this specialization. \n",
    "- Indeed most of the deep nets you will be implementing will have a similar structure. The only thing that changes is the model architecture, the inputs, and the outputs. Before starting the assignment, we will introduce you to the Google library `trax` that we use for building and training models.\n",
    "\n",
    "\n",
    "Now we will show you how to compute the gradient of a certain function `f` by just using `  .grad(f)`. \n",
    "\n",
    "- Trax source code can be found on Github: [Trax](https://github.com/google/trax)\n",
    "- The Trax code also uses the JAX library: [JAX](https://jax.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "\n",
    "# Part 1:  Import libraries and try out Trax\n",
    "\n",
    "- Let's import libraries and look at an example of using the Trax library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random as rnd\n",
    "\n",
    "import numpy as np\n",
    "import trax\n",
    "from   trax import layers as t\n",
    "from   trax.fastmath import grad\n",
    "import trax.fastmath.numpy as fnp\n",
    "\n",
    "from utils import get_all_tweets, Layer, load_tweets, process_tweet\n",
    "\n",
    "#trax.supervised.trainer_lib.init_random_number_generators(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray(5., dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "jax.interpreters.xla.DeviceArray"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = fnp.array(5.)\n",
    "display(a)\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(a) for a=5.0: 25.0\n"
     ]
    }
   ],
   "source": [
    "print(f'f(a) for a={a}: {f(a)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient (derivative) of function `f` with respect to its input `x` is the derivative of $x^2$.\n",
    "- The derivative of $x^2$ is $2x$.  \n",
    "- When x is 5, then $2x=10$.\n",
    "\n",
    "You can calculate the gradient of a function by using `trax.fastmath.grad(fun=)` and passing in the name of the function.\n",
    "- In this case the function you want to take the gradient of is `f`.\n",
    "- The object returned (saved in `grad_f` in this example) is a function that can calculate the gradient of f for a given trax.fastmath.numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_f = grad(fun=f)\n",
    "type(grad_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(10., dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grad_calculation = grad_f(a) # f = x^2 -> d/dx = 2x\n",
    "display(grad_calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2:  Importing the data\n",
    "\n",
    "## 2.1  Loading in the data\n",
    "\n",
    "Import the data set.  \n",
    "- You may recognize this from earlier assignments in the specialization.\n",
    "- Details of process_tweet function are available in utils.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = '../../../../data/twitter_samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = get_all_tweets(DATA, 'positive')\n",
    "all_negative_tweets = get_all_tweets(DATA, 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_positive_tweets), len(all_negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_TRAIN = 4000\n",
    "train_pos = all_positive_tweets[:N_TRAIN]\n",
    "val_pos = all_positive_tweets[N_TRAIN:]\n",
    "train_neg = all_negative_tweets[:N_TRAIN]\n",
    "val_neg = all_negative_tweets[N_TRAIN:]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "val_x = val_pos + val_neg\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "val_y = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
    "\n",
    "len(train_x), len(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "processed: ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print('original:', train_pos[0])\n",
    "print('processed:', process_tweet(train_pos[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2  Building the vocabulary\n",
    "\n",
    "Now build the vocabulary.\n",
    "- Map each word in each tweet to an integer (an \"index\"). \n",
    "- The following code does this for you, but please read it and understand what it's doing.\n",
    "- Note that you will build the vocabulary based on the training data. \n",
    "- To do so, you will assign an index to everyword by iterating over your training set.\n",
    "\n",
    "The vocabulary will also include some special tokens\n",
    "- `__PAD__`: padding\n",
    "- `</e>`: end of line\n",
    "- `__UNK__`: a token representing any word that is not in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9111"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2}\n",
    "\n",
    "i = 3\n",
    "for tweet in train_x:\n",
    "    processed = process_tweet(tweet)\n",
    "    for word in processed:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = i\n",
    "            i += 1\n",
    "            \n",
    "len(vocab)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3  Converting a tweet to a tensor\n",
    "\n",
    "Write a function that will convert each tweet to a tensor (a list of unique integer IDs representing the processed tweet).\n",
    "- Note, the returned data type will be a **regular Python `list()`**\n",
    "    - You won't use TensorFlow in this function\n",
    "    - You also won't use a numpy array\n",
    "    - You also won't use trax.fastmath.numpy array\n",
    "- For words in the tweet that are not in the vocabulary, set them to the unique ID for the token `__UNK__`.\n",
    "\n",
    "##### Example\n",
    "Input a tweet:\n",
    "```CPP\n",
    "'@happypuppy, is Maria happy?'\n",
    "```\n",
    "\n",
    "The tweet_to_tensor will first conver the tweet into a list of tokens (including only relevant words)\n",
    "```CPP\n",
    "['maria', 'happi']\n",
    "```\n",
    "\n",
    "Then it will convert each word into its unique integer\n",
    "\n",
    "```CPP\n",
    "[2, 56]\n",
    "```\n",
    "- Notice that the word \"maria\" is not in the vocabulary, so it is assigned the unique integer associated with the `__UNK__` token, because it is considered \"unknown.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 01\n",
    "**Instructions:** Write a program `tweet_to_tensor` that takes in a tweet and converts it to an array of numbers. You can use the `Vocab` dictionary you just found to help create the tensor. \n",
    "\n",
    "- Use the vocab_dict parameter and not a global variable.\n",
    "- Do not hard code the integer value for the `__UNK__` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet - A string containing a tweet\n",
    "        vocab_dict - The words dictionary\n",
    "        unk_token - The special string for unknown tokens\n",
    "        verbose - Print info durign runtime\n",
    "    Output:\n",
    "        tensor_l - A python list with\n",
    "        \n",
    "    '''  \n",
    "    word_l = process_tweet(tweet)\n",
    "    if verbose:\n",
    "        print('List of words from the processed tweet:')\n",
    "        print(word_l)\n",
    "    tensor_l = []\n",
    "    unk_ID = vocab_dict[unk_token]\n",
    "    if verbose:\n",
    "        print(f'The unique integer ID for the unk_token is {unk_ID}')\n",
    "    tensor_l = [vocab_dict.get(word, unk_ID) for word in word_l]    \n",
    "    return tensor_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual tweet is\n",
      " Bro:U wan cut hair anot,ur hair long Liao bo\n",
      "Me:since ord liao,take it easy lor treat as save $ leave it longer :)\n",
      "Bro:LOL Sibei xialan\n",
      "\n",
      "Tensor of tweet:\n",
      " [1076, 138, 486, 2365, 755, 8170, 1134, 755, 54, 2, 2688, 801, 2, 2, 355, 609, 2, 3507, 1028, 605, 4579, 9, 1076, 159, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print('Actual tweet is\\n', val_pos[0])\n",
    "print(\n",
    "    '\\nTensor of tweet:\\n', tweet_to_tensor(val_pos[0], vocab_dict=vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4  Creating a batch generator\n",
    "\n",
    "Most of the time in Natural Language Processing, and AI in general we use batches when training our data sets. \n",
    "- If instead of training with batches of examples, you were to train a model with one example at a time, it would take a very long time to train the model. \n",
    "- You will now build a data generator that takes in the positive/negative tweets and returns a batch of training examples. It returns the model inputs, the targets (positive or negative labels) and the weight for each target (ex: this allows us to can treat some examples as more important to get right than others, but commonly this will all be 1.0). \n",
    "\n",
    "Once you create the generator, you could include it in a for loop\n",
    "\n",
    "```CPP\n",
    "for batch_inputs, batch_targets, batch_example_weights in data_generator:\n",
    "    ...\n",
    "```\n",
    "\n",
    "You can also get a single batch like this:\n",
    "\n",
    "```CPP\n",
    "batch_inputs, batch_targets, batch_example_weights = next(data_generator)\n",
    "```\n",
    "The generator returns the next batch each time it's called. \n",
    "- This generator returns the data in a format (tensors) that you could directly use in your model.\n",
    "- It returns a triple: the inputs, targets, and loss weights:\n",
    "-- Inputs is a tensor that contains the batch of tweets we put into the model.\n",
    "-- Targets is the corresponding batch of labels that we train to generate.\n",
    "-- Loss weights here are just 1s with same shape as targets. Next week, you will use it to mask input padding.\n",
    "\n",
    "### Exercise 02\n",
    "Implement `data_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(\n",
    "        data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
    "    '''\n",
    "    Input: \n",
    "      data_pos - Set of posstive examples\n",
    "      data_neg - Set of negative examples\n",
    "      batch_size - number of samples per batch. Must be even\n",
    "      loop - True or False\n",
    "      vocab_dict - The words dictionary\n",
    "      shuffle - Shuffle the data order\n",
    "    Yield:\n",
    "      inputs - Subset of positive and negative examples\n",
    "      targets - The corresponding labels for the subset\n",
    "      example_weights - An array specifying the importance of each example  \n",
    "    '''     \n",
    "    ### START GIVEN CODE ###\n",
    "    # make sure the batch size is an even number\n",
    "    # to allow an equal number of positive and negative samples\n",
    "    assert batch_size % 2 == 0\n",
    "    \n",
    "    # Number of positive examples in each batch is half of the batch size\n",
    "    # same with number of negative examples in each batch\n",
    "    n_to_take = batch_size // 2\n",
    "    \n",
    "    # Use pos_index to walk through the data_pos array\n",
    "    # same with neg_index and data_neg\n",
    "    pos_index = 0\n",
    "    neg_index = 0\n",
    "    len_data_pos = len(data_pos)\n",
    "    len_data_neg = len(data_neg)\n",
    "    \n",
    "    # Get and array with the data indexes\n",
    "    pos_index_lines = list(range(len_data_pos))\n",
    "    neg_index_lines = list(range(len_data_neg))\n",
    "    \n",
    "    # shuffle lines if shuffle is set to True\n",
    "    if shuffle:\n",
    "        rnd.shuffle(pos_index_lines)\n",
    "        rnd.shuffle(neg_index_lines)\n",
    "        \n",
    "    stop = False\n",
    "    \n",
    "    # Loop indefinitely\n",
    "    while not stop:  \n",
    "        # create a batch with positive and negative examples\n",
    "        batch = []\n",
    "        # First part: Pack n_to_take positive examples\n",
    "        # Start from pos_index and increment i up to n_to_take\n",
    "        for i in range(n_to_take):\n",
    "            # If the positive index goes past the positive dataset length\n",
    "            if pos_index >= len_data_pos: \n",
    "                # If loop is set to False, break once we reach the end of\n",
    "                # the dataset\n",
    "                if not loop:\n",
    "                    stop = True\n",
    "                    break\n",
    "                \n",
    "                # If user wants to keep re-using the data, reset the index\n",
    "                pos_index = 0\n",
    "                if shuffle:\n",
    "                    # Shuffle the index of the positive sample\n",
    "                    rnd.shuffle(pos_index_lines)\n",
    "                    \n",
    "            # get the tweet as pos_index\n",
    "            tweet = data_pos[pos_index_lines[pos_index]]\n",
    "            # convert the tweet into tensors of integers representing the\n",
    "            # processed words\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            # append the tensor to the batch list\n",
    "            batch.append(tensor)\n",
    "            # Increment pos_index by one\n",
    "            pos_index += 1\n",
    "        ### END GIVEN CODE ###\n",
    "            \n",
    "        ### START CODE HERE (Replace instances of 'None' with your code)\n",
    "        # Second part: Pack n_to_take negative examples\n",
    "        # Using the same batch list, start from neg_index and increment i \n",
    "        # up to n_to_take\n",
    "        for i in range(n_to_take):            \n",
    "            # If the negative index goes past the negative dataset length\n",
    "            if neg_index >= len_data_neg\n",
    "                # If loop is set to False, break once we reach the end of\n",
    "                #the dataset\n",
    "                if not loop:\n",
    "                    stop = True\n",
    "                    break\n",
    "                    \n",
    "                # If user wants to keep re-using the data, reset the index\n",
    "                neg_index = 0\n",
    "                if shuffle:\n",
    "                    # Shuffle the index of the negative sample\n",
    "                    rnd.shuffle(neg_index_lines)\n",
    "                    \n",
    "            # get the tweet as neg_index\n",
    "            tweet = data_neg[neg_index_lines[neg_index]]\n",
    "            # convert the tweet into tensors of integers representing the \n",
    "            # processed words\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            # append the tensor to the batch list\n",
    "            batch.append(tensor)\n",
    "            # Increment neg_index by one\n",
    "            neg_index += 1\n",
    "        ### END CODE HERE ###        \n",
    "\n",
    "        # -------------CONTINUE HERE------------------\n",
    "        ### START GIVEN CODE ###\n",
    "        if stop:\n",
    "            break;\n",
    "\n",
    "        # Update the start index for positive data \n",
    "        # so that it's n_to_take positions after the current pos_index\n",
    "        pos_index += n_to_take\n",
    "        \n",
    "        # Update the start index for negative data \n",
    "        # so that it's n_to_take positions after the current neg_index\n",
    "        neg_index += n_to_take\n",
    "        \n",
    "        # Get the max tweet length (the length of the longest tweet) \n",
    "        # (you will pad all shorter tweets to have this length)\n",
    "        max_len = max([len(t) for t in batch]) \n",
    "        \n",
    "        \n",
    "        # Initialize the input_l, which will \n",
    "        # store the padded versions of the tensors\n",
    "        tensor_pad_l = []\n",
    "        # Pad shorter tweets with zeros\n",
    "        for tensor in batch:\n",
    "### END GIVEN CODE ###\n",
    "\n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "            # Get the number of positions to pad for this tensor so that it will be max_len long\n",
    "            n_pad = None\n",
    "            \n",
    "            # Generate a list of zeros, with length n_pad\n",
    "            pad_l = None\n",
    "            \n",
    "            # concatenate the tensor and the list of padded zeros\n",
    "            tensor_pad = None\n",
    "            \n",
    "            # append the padded tensor to the list of padded tensors\n",
    "            None\n",
    "\n",
    "        # convert the list of padded tensors to a numpy array\n",
    "        # and store this as the model inputs\n",
    "        inputs = None\n",
    "  \n",
    "        # Generate the list of targets for the positive examples (a list of ones)\n",
    "        # The length is the number of positive examples in the batch\n",
    "        target_pos = None\n",
    "        \n",
    "        # Generate the list of targets for the negative examples (a list of zeros)\n",
    "        # The length is the number of negative examples in the batch\n",
    "        target_neg = None\n",
    "        \n",
    "        # Concatenate the positve and negative targets\n",
    "        target_l = None\n",
    "        \n",
    "        # Convert the target list into a numpy array\n",
    "        targets = None\n",
    "\n",
    "        # Example weights: Treat all examples equally importantly.It should return an np.array. Hint: Use np.ones_like()\n",
    "        example_weights = None\n",
    "        \n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "### GIVEN CODE ###\n",
    "        # note we use yield and not return\n",
    "        yield inputs, targets, example_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
