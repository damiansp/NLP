{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1:  Sentiment with Deep Neural Networks\n",
    "\n",
    "Welcome to the first assignment of course 3. In this assignment, you will explore sentiment analysis using deep neural networks. \n",
    "\n",
    "In course 1, you implemented Logistic regression and Naive Bayes for sentiment analysis. However if you were to give your old models an example like:\n",
    "\n",
    "<center> <span style='color:blue'> <b>This movie was almost good.</b> </span> </center>\n",
    "\n",
    "Your model would have predicted a positive sentiment for that review. However, that sentence has a negative sentiment and indicates that the movie was not good. To solve those kinds of misclassifications, you will write a program that uses deep neural networks to identify sentiment in text. By completing this assignment, you will: \n",
    "\n",
    "- Understand how you can build/design a model using layers\n",
    "- Train a model using a training loop\n",
    "- Use a binary cross-entropy loss function\n",
    "- Compute the accuracy of your model\n",
    "- Predict using your own input\n",
    "\n",
    "As you can tell, this model follows a similar structure to the one you previously implemented in the second course of this specialization. \n",
    "- Indeed most of the deep nets you will be implementing will have a similar structure. The only thing that changes is the model architecture, the inputs, and the outputs. Before starting the assignment, we will introduce you to the Google library `trax` that we use for building and training models.\n",
    "\n",
    "\n",
    "Now we will show you how to compute the gradient of a certain function `f` by just using `  .grad(f)`. \n",
    "\n",
    "- Trax source code can be found on Github: [Trax](https://github.com/google/trax)\n",
    "- The Trax code also uses the JAX library: [JAX](https://jax.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "\n",
    "# Part 1:  Import libraries and try out Trax\n",
    "\n",
    "- Let's import libraries and look at an example of using the Trax library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random as rnd\n",
    "\n",
    "import numpy as np\n",
    "import trax\n",
    "from   trax import layers as tl\n",
    "from   trax import fastmath\n",
    "from   trax.fastmath import grad\n",
    "import trax.fastmath.numpy as fnp\n",
    "from   trax.supervised import training\n",
    "\n",
    "from utils import get_all_tweets, Layer, load_tweets, process_tweet\n",
    "\n",
    "#trax.supervised.trainer_lib.init_random_number_generators(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray(5., dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "jax.interpreters.xla.DeviceArray"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = fnp.array(5.)\n",
    "display(a)\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(a) for a=5.0: 25.0\n"
     ]
    }
   ],
   "source": [
    "print(f'f(a) for a={a}: {f(a)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient (derivative) of function `f` with respect to its input `x` is the derivative of $x^2$.\n",
    "- The derivative of $x^2$ is $2x$.  \n",
    "- When x is 5, then $2x=10$.\n",
    "\n",
    "You can calculate the gradient of a function by using `trax.fastmath.grad(fun=)` and passing in the name of the function.\n",
    "- In this case the function you want to take the gradient of is `f`.\n",
    "- The object returned (saved in `grad_f` in this example) is a function that can calculate the gradient of f for a given trax.fastmath.numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_f = grad(fun=f)\n",
    "type(grad_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(10., dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grad_calculation = grad_f(a) # f = x^2 -> d/dx = 2x\n",
    "display(grad_calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2:  Importing the data\n",
    "\n",
    "## 2.1  Loading in the data\n",
    "\n",
    "Import the data set.  \n",
    "- You may recognize this from earlier assignments in the specialization.\n",
    "- Details of process_tweet function are available in utils.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = '../../../../data/twitter_samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = get_all_tweets(DATA, 'positive')\n",
    "all_negative_tweets = get_all_tweets(DATA, 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_positive_tweets), len(all_negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_TRAIN = 4000\n",
    "train_pos = all_positive_tweets[:N_TRAIN]\n",
    "val_pos = all_positive_tweets[N_TRAIN:]\n",
    "train_neg = all_negative_tweets[:N_TRAIN]\n",
    "val_neg = all_negative_tweets[N_TRAIN:]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "val_x = val_pos + val_neg\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "val_y = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
    "\n",
    "len(train_x), len(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "processed: ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print('original:', train_pos[0])\n",
    "print('processed:', process_tweet(train_pos[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2  Building the vocabulary\n",
    "\n",
    "Now build the vocabulary.\n",
    "- Map each word in each tweet to an integer (an \"index\"). \n",
    "- The following code does this for you, but please read it and understand what it's doing.\n",
    "- Note that you will build the vocabulary based on the training data. \n",
    "- To do so, you will assign an index to everyword by iterating over your training set.\n",
    "\n",
    "The vocabulary will also include some special tokens\n",
    "- `__PAD__`: padding\n",
    "- `</e>`: end of line\n",
    "- `__UNK__`: a token representing any word that is not in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9111"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2}\n",
    "\n",
    "i = 3\n",
    "for tweet in train_x:\n",
    "    processed = process_tweet(tweet)\n",
    "    for word in processed:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = i\n",
    "            i += 1\n",
    "            \n",
    "len(vocab)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3  Converting a tweet to a tensor\n",
    "\n",
    "Write a function that will convert each tweet to a tensor (a list of unique integer IDs representing the processed tweet).\n",
    "- Note, the returned data type will be a **regular Python `list()`**\n",
    "    - You won't use TensorFlow in this function\n",
    "    - You also won't use a numpy array\n",
    "    - You also won't use trax.fastmath.numpy array\n",
    "- For words in the tweet that are not in the vocabulary, set them to the unique ID for the token `__UNK__`.\n",
    "\n",
    "##### Example\n",
    "Input a tweet:\n",
    "```CPP\n",
    "'@happypuppy, is Maria happy?'\n",
    "```\n",
    "\n",
    "The tweet_to_tensor will first conver the tweet into a list of tokens (including only relevant words)\n",
    "```CPP\n",
    "['maria', 'happi']\n",
    "```\n",
    "\n",
    "Then it will convert each word into its unique integer\n",
    "\n",
    "```CPP\n",
    "[2, 56]\n",
    "```\n",
    "- Notice that the word \"maria\" is not in the vocabulary, so it is assigned the unique integer associated with the `__UNK__` token, because it is considered \"unknown.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 01\n",
    "**Instructions:** Write a program `tweet_to_tensor` that takes in a tweet and converts it to an array of numbers. You can use the `Vocab` dictionary you just found to help create the tensor. \n",
    "\n",
    "- Use the vocab_dict parameter and not a global variable.\n",
    "- Do not hard code the integer value for the `__UNK__` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet - A string containing a tweet\n",
    "        vocab_dict - The words dictionary\n",
    "        unk_token - The special string for unknown tokens\n",
    "        verbose - Print info durign runtime\n",
    "    Output:\n",
    "        tensor_l - A python list with\n",
    "        \n",
    "    '''  \n",
    "    word_l = process_tweet(tweet)\n",
    "    if verbose:\n",
    "        print('List of words from the processed tweet:')\n",
    "        print(word_l)\n",
    "    tensor_l = []\n",
    "    unk_ID = vocab_dict[unk_token]\n",
    "    if verbose:\n",
    "        print(f'The unique integer ID for the unk_token is {unk_ID}')\n",
    "    tensor_l = [vocab_dict.get(word, unk_ID) for word in word_l]    \n",
    "    return tensor_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual tweet is\n",
      " Bro:U wan cut hair anot,ur hair long Liao bo\n",
      "Me:since ord liao,take it easy lor treat as save $ leave it longer :)\n",
      "Bro:LOL Sibei xialan\n",
      "\n",
      "Tensor of tweet:\n",
      " [1076, 138, 486, 2365, 755, 8170, 1134, 755, 54, 2, 2688, 801, 2, 2, 355, 609, 2, 3507, 1028, 605, 4579, 9, 1076, 159, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print('Actual tweet is\\n', val_pos[0])\n",
    "print(\n",
    "    '\\nTensor of tweet:\\n', tweet_to_tensor(val_pos[0], vocab_dict=vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4  Creating a batch generator\n",
    "\n",
    "Most of the time in Natural Language Processing, and AI in general we use batches when training our data sets. \n",
    "- If instead of training with batches of examples, you were to train a model with one example at a time, it would take a very long time to train the model. \n",
    "- You will now build a data generator that takes in the positive/negative tweets and returns a batch of training examples. It returns the model inputs, the targets (positive or negative labels) and the weight for each target (ex: this allows us to can treat some examples as more important to get right than others, but commonly this will all be 1.0). \n",
    "\n",
    "Once you create the generator, you could include it in a for loop\n",
    "\n",
    "```CPP\n",
    "for batch_inputs, batch_targets, batch_example_weights in data_generator:\n",
    "    ...\n",
    "```\n",
    "\n",
    "You can also get a single batch like this:\n",
    "\n",
    "```CPP\n",
    "batch_inputs, batch_targets, batch_example_weights = next(data_generator)\n",
    "```\n",
    "The generator returns the next batch each time it's called. \n",
    "- This generator returns the data in a format (tensors) that you could directly use in your model.\n",
    "- It returns a triple: the inputs, targets, and loss weights:\n",
    "-- Inputs is a tensor that contains the batch of tweets we put into the model.\n",
    "-- Targets is the corresponding batch of labels that we train to generate.\n",
    "-- Loss weights here are just 1s with same shape as targets. Next week, you will use it to mask input padding.\n",
    "\n",
    "### Exercise 02\n",
    "Implement `data_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(\n",
    "        data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
    "    '''\n",
    "    Input: \n",
    "      data_pos - Set of posstive examples\n",
    "      data_neg - Set of negative examples\n",
    "      batch_size - number of samples per batch. Must be even\n",
    "      loop - True or False\n",
    "      vocab_dict - The words dictionary\n",
    "      shuffle - Shuffle the data order\n",
    "    Yield:\n",
    "      inputs - Subset of positive and negative examples\n",
    "      targets - The corresponding labels for the subset\n",
    "      example_weights - An array specifying the importance of each example  \n",
    "    '''     \n",
    "    assert batch_size % 2 == 0\n",
    "    n_to_take = batch_size // 2\n",
    "    pos_index = 0\n",
    "    neg_index = 0\n",
    "    len_data_pos = len(data_pos)\n",
    "    len_data_neg = len(data_neg)\n",
    "    pos_index_lines = list(range(len_data_pos))\n",
    "    neg_index_lines = list(range(len_data_neg))\n",
    "    if shuffle:\n",
    "        rnd.shuffle(pos_index_lines)\n",
    "        rnd.shuffle(neg_index_lines)        \n",
    "    stop = False\n",
    "    while not stop:  \n",
    "        batch = []\n",
    "        for i in range(n_to_take):\n",
    "            if pos_index >= len_data_pos: \n",
    "                if not loop:\n",
    "                    stop = True\n",
    "                    break              \n",
    "                pos_index = 0\n",
    "                if shuffle:\n",
    "                    rnd.shuffle(pos_index_lines)\n",
    "            tweet = data_pos[pos_index_lines[pos_index]]\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            batch.append(tensor)\n",
    "            pos_index += 1\n",
    "        for i in range(n_to_take):            \n",
    "            if neg_index >= len_data_neg:\n",
    "                if not loop:\n",
    "                    stop = True\n",
    "                    break\n",
    "                neg_index = 0\n",
    "                if shuffle:\n",
    "                    rnd.shuffle(neg_index_lines)\n",
    "            tweet = data_neg[neg_index_lines[neg_index]]\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            batch.append(tensor)\n",
    "            neg_index += 1\n",
    "        if stop:\n",
    "            break\n",
    "        pos_index += n_to_take\n",
    "        neg_index += n_to_take\n",
    "        max_len = max([len(t) for t in batch]) \n",
    "        tensor_pad_l = []\n",
    "        for tensor in batch:\n",
    "            n_pad = max_len - len(tensor)\n",
    "            pad_l = [0] * n_pad\n",
    "            tensor_pad = tensor + pad_l\n",
    "            tensor_pad_l.append(tensor_pad)\n",
    "        inputs = np.array(tensor_pad_l)\n",
    "        targets = np.array([1] * n_to_take + [0] * n_to_take)\n",
    "        example_weights = np.ones_like(targets)\n",
    "        yield inputs, targets, example_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [[ 349 2019 4471 3218    9    0    0    0    0    0    0]\n",
      " [4974  575 2014 1467 5194 3517  143 3517  132  466    9]\n",
      " [3780  111  138  591 2946 3989    0    0    0    0    0]\n",
      " [ 253 3780    0    0    0    0    0    0    0    0    0]]\n",
      "Targets: [1 1 0 0]\n",
      "Example Weights: [1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Set the random number generator for the shuffle procedure\n",
    "rnd.seed(30) \n",
    "\n",
    "# Create the training data generator\n",
    "def train_generator(batch_size, shuffle=False):\n",
    "    return data_generator(\n",
    "        train_pos, train_neg, batch_size, True, vocab, shuffle)\n",
    "\n",
    "# Create the validation data generator\n",
    "def val_generator(batch_size, shuffle = False):\n",
    "    return data_generator(\n",
    "        val_pos, val_neg, batch_size, True, vocab, shuffle)\n",
    "\n",
    "# Create the validation data generator\n",
    "def test_generator(batch_size, shuffle = False):\n",
    "    return data_generator(\n",
    "        val_pos, val_neg, batch_size, False, vocab, shuffle)\n",
    "\n",
    "# Get a batch from the train_generator and inspect.\n",
    "inputs, targets, example_weights = next(train_generator(4, shuffle=True))\n",
    "\n",
    "# this will print a list of 4 tensors padded with zeros\n",
    "print(f'Inputs: {inputs}')\n",
    "print(f'Targets: {targets}')\n",
    "print(f'Example Weights: {example_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inputs shape is (4, 14)\n",
      "The targets shape is (4,)\n",
      "The example weights shape is (4,)\n",
      "input tensor: [3 4 5 6 7 8 9 0 0 0 0 0 0 0]; target 1; example weights 1\n",
      "input tensor: [10 11 12 13 14 15 16 17 18 19 20  9 21 22]; target 1; example weights 1\n",
      "input tensor: [5758 2917 3780    0    0    0    0    0    0    0    0    0    0    0]; target 0; example weights 1\n",
      "input tensor: [ 868  259 3670 5759  311 4478  575 1241 2783  333 1213 3780    0    0]; target 0; example weights 1\n"
     ]
    }
   ],
   "source": [
    "# Test the train_generator\n",
    "# Create a data generator for training data,\n",
    "# which produces batches of size 4 (for tensors and their respective \n",
    "# targets)\n",
    "tmp_data_gen = train_generator(batch_size = 4)\n",
    "\n",
    "# Call the data generator to get one batch and its targets\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = next(tmp_data_gen)\n",
    "\n",
    "print(f'The inputs shape is {tmp_inputs.shape}')\n",
    "print(f'The targets shape is {tmp_targets.shape}')\n",
    "print(f'The example weights shape is {tmp_example_weights.shape}')\n",
    "\n",
    "for i,t in enumerate(tmp_inputs):\n",
    "    print(f'input tensor: {t}; target {tmp_targets[i]}; '\n",
    "          f'example weights {tmp_example_weights[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have your train/val generators, you can just call them and they will return tensors which correspond to your tweets in the first column and their corresponding labels in the second column. Now you can go ahead and start building your neural network. \n",
    "\n",
    "# Part 3:  Defining classes\n",
    "\n",
    "In this part, you will write your own library of layers. It will be very similar\n",
    "to the one used in Trax and also in Keras and PyTorch. Writing your own small\n",
    "framework will help you understand how they all work and use them effectively\n",
    "in the future.\n",
    "\n",
    "Your framework will be based on the following `Layer` class from utils.py.\n",
    "\n",
    "```CPP\n",
    "class Layer(object):\n",
    "    \"\"\" Base class for layers.\n",
    "    \"\"\"\n",
    "      \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        # set weights to None\n",
    "        self.weights = None\n",
    "\n",
    "    # The forward propagation should be implemented\n",
    "    # by subclasses of this Layer class\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # This function initializes the weights\n",
    "    # based on the input signature and random key,\n",
    "    # should be implemented by subclasses of this Layer class\n",
    "    def init_weights_and_state(self, input_signature, random_key):\n",
    "        pass\n",
    "\n",
    "    # This initializes and returns the weights, do not override.\n",
    "    def init(self, input_signature, random_key):\n",
    "        self.init_weights_and_state(input_signature, random_key)\n",
    "        return self.weights\n",
    " \n",
    "    # __call__ allows an object of this class\n",
    "    # to be called like it's a function.\n",
    "    def __call__(self, x):\n",
    "        # When this layer object is called, \n",
    "        # it calls its forward propagation function\n",
    "        return self.forward(x)\n",
    "```\n",
    "\n",
    "## 3.1  ReLU class\n",
    "You will now implement the ReLU activation function in a class below.\n",
    "$$ \\mathrm{ReLU}(x) = \\mathrm{max}(0,x) $$\n",
    "\n",
    "### Exercise 03\n",
    "**Instructions:** Implement the ReLU activation function below. Your function should take in a matrix or vector and it should transform all the negative numbers into 0 while keeping all the positive numbers intact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Layer):\n",
    "    \"\"\"Relu activation function implementation\"\"\"\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Input: \n",
    "            - x (a numpy array): the input\n",
    "        Output:\n",
    "            - activation (numpy array): all positive or 0 version of x\n",
    "        '''\n",
    "        activation = x.copy()\n",
    "        activation[activation < 0] = 0\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data is:\n",
      "[[-2. -1.  0.]\n",
      " [ 0.  1.  2.]]\n",
      "Output of Relu is:\n",
      "[[0. 0. 0.]\n",
      " [0. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "# Test your relu function\n",
    "x = np.array([[-2.0, -1.0, 0.0], [0.0, 1.0, 2.0]], dtype=float)\n",
    "relu_layer = Relu()\n",
    "print(\"Test data is:\")\n",
    "print(x)\n",
    "print(\"Output of Relu is:\")\n",
    "print(relu_layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2  Dense class \n",
    "\n",
    "### Exercise\n",
    "\n",
    "Implement the forward function of the Dense class. \n",
    "- The forward function multiplies the input to the layer (`x`) by the weight matrix (`W`)\n",
    "\n",
    "$$\\mathrm{forward}(\\mathbf{x},\\mathbf{W}) = \\mathbf{xW} $$\n",
    "\n",
    "- You can use `numpy.dot` to perform the matrix multiplication.\n",
    "\n",
    "Note that for more efficient code execution, you will use the trax version of `math`, which includes a trax version of `numpy` and also `random`.\n",
    "\n",
    "Implement the weight initializer `new_weights` function\n",
    "- Weights are initialized with a random key.\n",
    "- The second parameter is a tuple for the desired shape of the weights (num_rows, num_cols)\n",
    "- The num of rows for weights should equal the number of columns in x, because for forward propagation, you will multiply x times weights.\n",
    "\n",
    "Please use `trax.fastmath.random.normal(key, shape, dtype=tf.float32)` to generate random values for the weight matrix. The key difference between this function\n",
    "and the standard `numpy` randomness is the explicit use of random keys, which\n",
    "need to be passed. While it can look tedious at the first sight to pass the random key everywhere, you will learn in Course 4 why this is very helpful when\n",
    "implementing some advanced models.\n",
    "- `key` can be generated by calling `random.get_prng(seed=)` and passing in a number for the `seed`.\n",
    "- `shape` is a tuple with the desired shape of the weight matrix.\n",
    "    - The number of rows in the weight matrix should equal the number of columns in the variable `x`.  Since `x` may have 2 dimensions if it reprsents a single training example (row, col), or three dimensions (batch_size, row, col), get the last dimension from the tuple that holds the dimensions of x.\n",
    "    - The number of columns in the weight matrix is the number of units chosen for that dense layer.  Look at the `__init__` function to see which variable stores the number of units.\n",
    "- `dtype` is the data type of the values in the generated matrix; keep the default of `tf.float32`. In this case, don't explicitly set the dtype (just let it use the default value).\n",
    "\n",
    "Set the standard deviation of the random values to 0.1\n",
    "- The values generated have a mean of 0 and standard deviation of 1.\n",
    "- Set the default standard deviation `stdev` to be 0.1 by multiplying the standard deviation to each of the values in the weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = fastmath.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random seed generated by random.get_prng\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([0, 1], dtype=uint32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose a matrix with 2 rows and 3 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight matrix generated with a normal distribution with mean 0 and stdev of 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.957307  , -0.9699291 ,  1.0070664 ],\n",
       "             [ 0.36619022,  0.17294823,  0.29092228]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# See how the fastmath.trax.random.normal function works\n",
    "tmp_key = random.get_prng(seed=1)\n",
    "print('The random seed generated by random.get_prng')\n",
    "display(tmp_key)\n",
    "\n",
    "print('choose a matrix with 2 rows and 3 columns')\n",
    "tmp_shape=(2, 3)\n",
    "display(tmp_shape)\n",
    "\n",
    "# Generate a weight matrix\n",
    "# Note that you'll get an error if you try to set dtype to tf.float32, \n",
    "# where tf is tensorflow\n",
    "# Just avoid setting the dtype and allow it to use the default data type\n",
    "tmp_weight = trax.fastmath.random.normal(key=tmp_key, shape=tmp_shape)\n",
    "\n",
    "print('Weight matrix generated with a normal distribution with mean 0 and'\n",
    "      ' stdev of 1')\n",
    "display(tmp_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"\n",
    "    A dense (fully-connected) layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_units, init_stdev=0.1):\n",
    "        self._n_units = n_units\n",
    "        self._init_stdev = init_stdev\n",
    "\n",
    "    def forward(self, x):\n",
    "        dense = fnp.dot(x, self.weights)\n",
    "        return dense\n",
    "\n",
    "    def init_weights_and_state(self, input_signature, random_key):\n",
    "        input_shape = input_signature.shape\n",
    "        shape = (input_shape[1], self._n_units)\n",
    "        w = trax.fastmath.random.normal(key=random_key, shape=shape)\n",
    "        w *= self._init_stdev\n",
    "        self.weights = w\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are\n",
      " [[-0.02837108  0.09368162 -0.10050076  0.14165013  0.10543301  0.09108126\n",
      "  -0.04265672  0.0986188  -0.05575325  0.00153249]\n",
      " [-0.20785688  0.0554837   0.09142365  0.05744595  0.07227863  0.01210617\n",
      "  -0.03237354  0.16234995  0.02450038 -0.13809784]\n",
      " [-0.06111237  0.01403724  0.08410042 -0.1094358  -0.10775021 -0.11396459\n",
      "  -0.05933381 -0.01557652 -0.03832145 -0.11144515]]\n",
      "Foward function output is [[-3.0395496   0.9266802   2.5414743  -2.050473   -1.9769388  -2.582209\n",
      "  -1.7952735   0.94427425 -0.8980402  -3.7497487 ]]\n"
     ]
    }
   ],
   "source": [
    "# Testing your Dense layer \n",
    "dense_layer = Dense(n_units=10)      # sets number of units in dense layer\n",
    "random_key = random.get_prng(seed=0) # sets random seed\n",
    "z = np.array([[2.0, 7.0, 25.0]])     # input array \n",
    "\n",
    "dense_layer.init(z, random_key)\n",
    "# Returns randomly generated weights\n",
    "print(f'Weights are\\n {dense_layer.weights}')\n",
    "# Returns multiplied values of units and weights\n",
    "print('Foward function output is', dense_layer(z)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3  Model\n",
    "\n",
    "Now you will implement a classifier using neural networks. Here is the model architecture you will be implementing. \n",
    "\n",
    "Embedding -> Dense(n) -> Dense(2) -> softmax -> preds\n",
    "\n",
    "For the model implementation, you will use the Trax layers library `tl`.\n",
    "Note that the second character of `tl` is the lowercase of letter `L`, not the number 1. Trax layers are very similar to the ones you implemented above,\n",
    "but in addition to trainable weights also have a non-trainable state.\n",
    "State is used in layers like batch normalization and for inference, you will learn more about it in course 4.\n",
    "\n",
    "First, look at the code of the Trax Dense layer and compare to your implementation above.\n",
    "- [tl.Dense](https://github.com/google/trax/blob/master/trax/layers/core.py#L29): Trax Dense layer implementation\n",
    "\n",
    "One other important layer that you will use a lot is one that allows to execute one layer after another in sequence.\n",
    "- [tl.Serial](https://github.com/google/trax/blob/master/trax/layers/combinators.py#L26): Combinator that applies layers serially.  \n",
    "    - You can pass in the layers as arguments to `Serial`, separated by commas. \n",
    "    - For example: `tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))`\n",
    "\n",
    "Please use the `help` function to view documentation for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [tl.Embedding](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L113): Layer constructor function for an embedding layer.  \n",
    "    - `tl.Embedding(vocab_size, d_feature)`.\n",
    "    - `vocab_size` is the number of unique words in the given vocabulary.\n",
    "    - `d_feature` is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(tl.Dense)\n",
    "#help(tl.Serial)\n",
    "#help(tl.Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding_3_2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_embed = tl.Embedding(vocab_size=3, d_feature=2)\n",
    "display(tmp_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [tl.Mean](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L276): Calculates means across an axis.  In this case, please choose axis = 1 to get an average embedding vector (an embedding vector that is an average of all words in the vocabulary).  \n",
    "- For example, if the embedding matrix is 300 elements and vocab size is 10,000 words, taking the mean of the embedding matrix along axis=1 will yield a vector of 300 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function Mean in module trax.layers.core:\n",
      "\n",
      "Mean(axis=-1, keepdims=False)\n",
      "    Returns a layer that computes mean values using one tensor axis.\n",
      "    \n",
      "    `Mean` uses one tensor axis to form groups of values and replaces each group\n",
      "    with the mean value of that group. The resulting values can either remain\n",
      "    in their own size 1 axis (`keepdims=True`), or that axis can be removed from\n",
      "    the overall tensor (default `keepdims=False`), lowering the rank of the\n",
      "    tensor by one.\n",
      "    \n",
      "    Args:\n",
      "      axis: Axis along which values are grouped for computing a mean.\n",
      "      keepdims: If `True`, keep the resulting size 1 axis as a separate tensor\n",
      "          axis; else, remove that axis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tl.Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.5, 3.5, 4.5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([2., 5.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Given the embedding matrix uses 2 elements for embedding meaning, and \n",
    "# has a vocab size of 3: shape = (2, 3)\n",
    "tmp_embed = np.array([[1, 2, 3], \n",
    "                      [4, 5, 6]])\n",
    "display(np.mean(tmp_embed, axis=0)) # means for each word\n",
    "display(np.mean(tmp_embed, axis=1)) # means for each semantic value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [tl.LogSoftmax](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L242): Implements log softmax function\n",
    "- Here, you don't need to set any parameters for `LogSoftMax()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(tl.LogSoftmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Online documentation**\n",
    "\n",
    "- [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense)\n",
    "\n",
    "- [tl.Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.combinators)\n",
    "\n",
    "- [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding)\n",
    "\n",
    "- [tl.Mean](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Mean)\n",
    "\n",
    "- [tl.LogSoftmax](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax)\n",
    "\n",
    "### Exercise 05\n",
    "Implement the classifier function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(\n",
    "        vocab_size=len(vocab), embedding_dim=256, output_dim=2, \n",
    "        mode='train'): \n",
    "    model = tl.Serial(tl.Embedding(vocab_size=vocab_size, \n",
    "                                   d_feature=embedding_dim),\n",
    "                      tl.Mean(axis=0),\n",
    "                      tl.Dense(n_units=output_dim),\n",
    "                      tl.LogSoftmax())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'trax.layers.combinators.Serial'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Serial[\n",
       "  Embedding_9111_256\n",
       "  Mean\n",
       "  Dense_2\n",
       "  LogSoftmax\n",
       "]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_model = classifier()\n",
    "print(type(tmp_model))\n",
    "display(tmp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4:  Training\n",
    "\n",
    "To train a model on a task, Trax defines an abstraction [`trax.supervised.training.TrainTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) which packages the train data, loss and optimizer (among other things) together into an object.\n",
    "\n",
    "Similarly to evaluate a model, Trax defines an abstraction [`trax.supervised.training.EvalTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) which packages the eval data and metrics (among other things) into another object.\n",
    "\n",
    "The final piece tying things together is the [`trax.supervised.training.Loop`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) abstraction that is a very simple and flexible way to put everything together and train the model, all the while evaluating it and saving checkpoints.\n",
    "Using `Loop` will save you a lot of code compared to always writing the training loop by hand, like you did in courses 1 and 2. More importantly, you are less likely to have a bug in that code that would ruin your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(trax.supervised.training.TrainTask)\n",
    "#help(trax.supervised.training.EvalTask)\n",
    "#help(trax.supervised.training.Loop)\n",
    "#help(trax.optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice some available optimizers include:\n",
    "```CPP\n",
    "    adafactor\n",
    "    adam\n",
    "    momentum\n",
    "    rms_prop\n",
    "    sm3\n",
    "```\n",
    "\n",
    "## 4.1  Training the model\n",
    "\n",
    "Now you are going to train your model. \n",
    "\n",
    "Let's define the `TrainTask`, `EvalTask` and `Loop` in preparation to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "ETA = 0.01\n",
    "rnd.seed(271)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task = training.TrainTask(\n",
    "    labeled_data=train_generator(batch_size=BATCH_SIZE, shuffle=True),\n",
    "    loss_layer=tl.CrossEntropyLoss(),\n",
    "    optimizer=trax.optimizers.Adam(ETA),\n",
    "    n_steps_per_checkpoint=10)\n",
    "eval_task = training.EvalTask(\n",
    "    labeled_data=val_generator(batch_size=BATCH_SIZE, shuffle=True),\n",
    "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()])\n",
    "model = classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines a model trained using [`tl.CrossEntropyLoss`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss) optimized with the [`trax.optimizers.Adam`](https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam) optimizer, all the while tracking the accuracy using [`tl.Accuracy`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.Accuracy) metric. We also track `tl.CrossEntropyLoss` on the validation set.\n",
    "\n",
    "Now let's make an output directory and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 06\n",
    "**Instructions:** Implement `train_model` to train the model (`classifier` that you wrote earlier) for the given number of training steps (`n_steps`) using `TrainTask`, `EvalTask` and `Loop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
    "    '''\n",
    "    Input: \n",
    "        classifier - the model you are building\n",
    "        train_task - Training task\n",
    "        eval_task - Evaluation task\n",
    "        n_steps - the evaluation steps\n",
    "        output_dir - folder to save your files\n",
    "    Output:\n",
    "        trainer -  trax trainer\n",
    "    '''\n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    training_loop = training.Loop(\n",
    "        classifier, # The learning model\n",
    "        train_task, # The training task\n",
    "        eval_task=eval_task, # The evaluation task\n",
    "        output_dir=output_dir) # The output directory\n",
    "    training_loop.run(n_steps=n_steps)\n",
    "### END CODE HERE ###\n",
    "\n",
    "    # Return the training_loop, since it has the model.\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'eval_task'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-9beb86f88367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m training_loop = train_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     model, train_task, eval_task, 100, output_dir)\n",
      "\u001b[0;32m<ipython-input-47-3361aa563206>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(classifier, train_task, eval_task, n_steps, output_dir)\u001b[0m\n\u001b[1;32m     11\u001b[0m     '''\n\u001b[1;32m     12\u001b[0m \u001b[0;31m### START CODE HERE (Replace instances of 'None' with your code) ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     training_loop = training.Loop(\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# The learning model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtrain_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# The training task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'eval_task'"
     ]
    }
   ],
   "source": [
    "training_loop = train_model(\n",
    "    model, train_task, eval_task, 100, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
