{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1:  Sentiment with Deep Neural Networks\n",
    "\n",
    "Welcome to the first assignment of course 3. In this assignment, you will explore sentiment analysis using deep neural networks. \n",
    "\n",
    "In course 1, you implemented Logistic regression and Naive Bayes for sentiment analysis. However if you were to give your old models an example like:\n",
    "\n",
    "<center> <span style='color:blue'> <b>This movie was almost good.</b> </span> </center>\n",
    "\n",
    "Your model would have predicted a positive sentiment for that review. However, that sentence has a negative sentiment and indicates that the movie was not good. To solve those kinds of misclassifications, you will write a program that uses deep neural networks to identify sentiment in text. By completing this assignment, you will: \n",
    "\n",
    "- Understand how you can build/design a model using layers\n",
    "- Train a model using a training loop\n",
    "- Use a binary cross-entropy loss function\n",
    "- Compute the accuracy of your model\n",
    "- Predict using your own input\n",
    "\n",
    "As you can tell, this model follows a similar structure to the one you previously implemented in the second course of this specialization. \n",
    "- Indeed most of the deep nets you will be implementing will have a similar structure. The only thing that changes is the model architecture, the inputs, and the outputs. Before starting the assignment, we will introduce you to the Google library `trax` that we use for building and training models.\n",
    "\n",
    "\n",
    "Now we will show you how to compute the gradient of a certain function `f` by just using `  .grad(f)`. \n",
    "\n",
    "- Trax source code can be found on Github: [Trax](https://github.com/google/trax)\n",
    "- The Trax code also uses the JAX library: [JAX](https://jax.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "\n",
    "# Part 1:  Import libraries and try out Trax\n",
    "\n",
    "- Let's import libraries and look at an example of using the Trax library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random as rnd\n",
    "\n",
    "import trax\n",
    "from   trax import layers as t\n",
    "from   trax.fastmath import grad\n",
    "import trax.fastmath.numpy as np\n",
    "\n",
    "from utils import Layer, load_tweets, process_tweet\n",
    "\n",
    "#trax.supervised.trainer_lib.init_random_number_generators(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray(5., dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "jax.interpreters.xla.DeviceArray"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(5.)\n",
    "display(a)\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(a) for a=5.0: 25.0\n"
     ]
    }
   ],
   "source": [
    "print(f'f(a) for a={a}: {f(a)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient (derivative) of function `f` with respect to its input `x` is the derivative of $x^2$.\n",
    "- The derivative of $x^2$ is $2x$.  \n",
    "- When x is 5, then $2x=10$.\n",
    "\n",
    "You can calculate the gradient of a function by using `trax.fastmath.grad(fun=)` and passing in the name of the function.\n",
    "- In this case the function you want to take the gradient of is `f`.\n",
    "- The object returned (saved in `grad_f` in this example) is a function that can calculate the gradient of f for a given trax.fastmath.numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_f = grad(fun=f)\n",
    "type(grad_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(10., dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grad_calculation = grad_f(a) # f = x^2 -> d/dx = 2x\n",
    "display(grad_calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2:  Importing the data\n",
    "\n",
    "## 2.1  Loading in the data\n",
    "\n",
    "Import the data set.  \n",
    "- You may recognize this from earlier assignments in the specialization.\n",
    "- Details of process_tweet function are available in utils.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
