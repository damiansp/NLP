{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "\n",
    "#nltk.download('punkt')    # sentence tokenizer: downloader broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.environ['HOME']\n",
    "punkt_path = f'{HOME}/nltk_data/tokenizers/punkt/PY3/english.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(punkt_path, 'rb') as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning% makes 'me' happy. i am happy be-cause i am learning! :)\n"
     ]
    }
   ],
   "source": [
    "corpus = (\"Learning% makes 'me' happy. I am happy be-cause I am \"\n",
    "          \"learning! :)\")\n",
    "corpus = corpus.lower()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning makes me happy. i am happy because i am learning! \n"
     ]
    }
   ],
   "source": [
    "corpus = (\"learning% makes 'me' happy. i am happy be-cause i am \"\n",
    "          \"learning! :)\")\n",
    "corpus = re.sub(r\"[^a-zA-Z0-9.?! ]+\", \"\", corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date parts = ['Sat', 'May', '', '9', '07:33:35', 'CEST', '2020']\n",
      "time parts = ['07', '33', '35']\n"
     ]
    }
   ],
   "source": [
    "input_date = 'Sat May  9 07:33:35 CEST 2020'\n",
    "date_parts = input_date.split(' ')\n",
    "print(f'date parts = {date_parts}')\n",
    "\n",
    "time_parts = date_parts[4].split(':')\n",
    "print(f'time parts = {time_parts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am happy because i am learning. -> ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'i am happy because i am learning.'\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "print(f'{sentence} -> {tokenized_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lengths of the words: \n",
      "[('i', 1), ('am', 2), ('happy', 5), ('because', 7), ('i', 1), ('am', 2), ('learning', 8), ('.', 1)]\n"
     ]
    }
   ],
   "source": [
    "sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
    "word_lengths = [(word, len(word)) for word in sentence]\n",
    "print(f' Lengths of the words: \\n{word_lengths}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all trigrams of sentence: ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
      "\n",
      "['i', 'am', 'happy']\n",
      "['am', 'happy', 'because']\n",
      "['happy', 'because', 'i']\n",
      "['because', 'i', 'am']\n",
      "['i', 'am', 'learning']\n",
      "['am', 'learning', '.']\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_trigram(tokenized_sentence):\n",
    "    for i in range(len(tokenized_sentence) - 3 + 1):\n",
    "        trigram = tokenized_sentence[i : i + 3]\n",
    "        print(trigram)\n",
    "\n",
    "tokenized_sentence = [\n",
    "    'i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
    "print(f'List all trigrams of sentence: {tokenized_sentence}\\n')\n",
    "sentence_to_trigram(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'happy']\n"
     ]
    }
   ],
   "source": [
    "fourgram = ['i', 'am', 'happy','because']\n",
    "trigram = fourgram[0:-1]\n",
    "print(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "tokenized_sentence = [\n",
    "    'i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
    "tokenized_sentence = ['<s>'] * (n - 1) + tokenized_sentence + ['</s>']\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
