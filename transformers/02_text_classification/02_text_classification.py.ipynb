{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "public-graham",
   "metadata": {},
   "source": [
    "See <a href=\"https://huggingface.co/datasets\">Huggingface Datasets</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from   datasets import list_datasets, load_dataset\n",
    "from   huggingface_hub import notebook_login\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from   sklearn.dummy import DummyClassifier\n",
    "from   sklearn.linear_model import LogisticRegression\n",
    "from   sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score)\n",
    "from   sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from   transformers import (\n",
    "    AutoModel, AutoModelForSequenceClassification, AutoTokenizer,\n",
    "    DistilBertTokenizer, TFAutoModel)\n",
    "#from umap import UMAP # no M1 Wheel yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "healthy-kuwait",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2329 datsets available, including\n",
      "  - 0n1xus/codexglue\n",
      "  - 0n1xus/pytorrent-standalone\n",
      "  - AConsApart/anime_subtitles_DialoGPT\n",
      "  - AI-Sweden/SuperLim\n",
      "  - AI-it/khs_service_test\n",
      "  - AI-it/korean-hate-speech\n",
      "  - ARKseal/YFCC14M_subset_webdataset\n",
      "  - ARTeLab/fanpage\n",
      "  - ARTeLab/ilpost\n",
      "  - ARTeLab/mlsum-it\n"
     ]
    }
   ],
   "source": [
    "datasets = list_datasets()\n",
    "print(f'There are {len(datasets)} datsets available, including')\n",
    "for i in range(10):\n",
    "    print(f'  - {datasets[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brazilian-mercy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. For more detailed information please refer to the paper.\n",
      "\n",
      "@inproceedings{saravia-etal-2018-carer,\n",
      "    title = \"{CARER}: Contextualized Affect Representations for Emotion Recognition\",\n",
      "    author = \"Saravia, Elvis  and\n",
      "      Liu, Hsien-Chi Toby  and\n",
      "      Huang, Yen-Hao  and\n",
      "      Wu, Junlin  and\n",
      "      Chen, Yi-Shin\",\n",
      "    booktitle = \"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\",\n"
     ]
    }
   ],
   "source": [
    "metadata = list_datasets(with_details=True)[datasets.index('emotion')]\n",
    "print(metadata.description)\n",
    "print()\n",
    "print('\\n'.join(metadata.citation.split('\\n')[:8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "premium-start",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset emotion (/Users/damiansp/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248585aa39584e2fb15921f43e127ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = load_dataset('emotion')\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "speaking-bunch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = emotions['train']\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "occupational-baker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sunrise-links",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i didnt feel humiliated', 'label': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "medieval-gender",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'label']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "legislative-defeat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=6, names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], names_file=None, id=None)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "brutal-express",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n",
       "  'im grabbing a minute to post i feel greedy wrong',\n",
       "  'i am ever feeling nostalgic about the fireplace i will know that it is still on the property',\n",
       "  'i am feeling grouchy',\n",
       "  'ive been feeling a little burdened lately wasnt sure why that was'],\n",
       " 'label': [0, 3, 2, 3, 0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "increasing-dividend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                            i didnt feel humiliated      0\n",
       "1  i can go from feeling so hopeless to so damned...      0\n",
       "2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3  i am ever feeling nostalgic about the fireplac...      2\n",
       "4                               i am feeling grouchy      3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions.set_format(type='pandas')\n",
    "df = emotions['train'][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "535715f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_int2str(i):\n",
    "    return emotions['train'].features['label'].int2str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bed397ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_name\n",
       "0                            i didnt feel humiliated      0    sadness\n",
       "1  i can go from feeling so hopeless to so damned...      0    sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong      3      anger\n",
       "3  i am ever feeling nostalgic about the fireplac...      2       love\n",
       "4                               i am feeling grouchy      3      anger"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label_name'] = df.label.apply(label_int2str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0d6064d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.label_name.value_counts(ascending=True).plot.barh()\n",
    "plt.title('Freq of Classes');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cb8d3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['words_per_tweet'] = df.text.str.split().apply(len)\n",
    "df.boxplot(\n",
    "    'words_per_tweet', by='label_name', showfliers=False, grid=False)\n",
    "plt.suptitle('')\n",
    "plt.xlabel('');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0ff490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436199f5",
   "metadata": {},
   "source": [
    "## From Text to Tokens\n",
    "### Character Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ef1ba56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 's', 'k', ' ', 'o', 'f', ' ', 'N', 'L', 'P', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'Tokenizing text is a core task of NLP.'\n",
    "tokenized_text = list(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a1242e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '.': 1,\n",
       " 'L': 2,\n",
       " 'N': 3,\n",
       " 'P': 4,\n",
       " 'T': 5,\n",
       " 'a': 6,\n",
       " 'c': 7,\n",
       " 'e': 8,\n",
       " 'f': 9,\n",
       " 'g': 10,\n",
       " 'i': 11,\n",
       " 'k': 12,\n",
       " 'n': 13,\n",
       " 'o': 14,\n",
       " 'r': 15,\n",
       " 's': 16,\n",
       " 't': 17,\n",
       " 'x': 18,\n",
       " 'z': 19}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx = {ch: idx \n",
    "             for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
    "token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc5b862d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 14, 12, 8, 13, 11, 19, 11, 13, 10, 0, 17, 8, 18, 17, 0, 11, 16, 0, 6, 0, 7, 14, 15, 8, 0, 17, 6, 16, 12, 0, 14, 9, 0, 3, 2, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "input_ids = [token2idx[token] for token in tokenized_text]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3710296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Megatron</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name  label\n",
       "0      Bumblebee      0\n",
       "1  Optimus Prime      1\n",
       "2       Megatron      2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_df = pd.DataFrame({\n",
    "    'name': ['Bumblebee', 'Optimus Prime', 'Megatron'],\n",
    "    'label': [0, 1, 2]})\n",
    "categorical_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bc6f4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bumblebee</th>\n",
       "      <th>Megatron</th>\n",
       "      <th>Optimus Prime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bumblebee  Megatron  Optimus Prime\n",
       "0          1         0              0\n",
       "1          0         0              1\n",
       "2          0         1              0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(categorical_df.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c76531fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 20])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor(input_ids)\n",
    "one_hot = F.one_hot(input_ids, num_classes=len(token2idx))\n",
    "one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6c113c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: T\n",
      "Index: 5\n",
      "One-Hot: tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(f'Token: {tokenized_text[0]}')\n",
    "print(f'Index: {input_ids[0]}')\n",
    "print(f'One-Hot: {one_hot[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a686b0",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad4d059f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenizing', 'text', 'is', 'a', 'core', 'task', 'of', 'NLP.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = text.split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8998c6",
   "metadata": {},
   "source": [
    "### Subword Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e97355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddf71b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent:\n",
    "#tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2945e4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tokenizing text is a core task of NLP.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f47e010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = tokenizer(text)\n",
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c14aa424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'token',\n",
       " '##izing',\n",
       " 'text',\n",
       " 'is',\n",
       " 'a',\n",
       " 'core',\n",
       " 'task',\n",
       " 'of',\n",
       " 'nl',\n",
       " '##p',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "085e6ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] tokenizing text is a core task of nlp. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94b91767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c30e5600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12e7e2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cad6f6",
   "metadata": {},
   "source": [
    "### Tokenizing the Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "359ea863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4206a20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['i didnt feel humiliated',\n",
       "  'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake'],\n",
       " 'label': [0, 0]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions['train'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb75301f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(emotions['train'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d8519b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/damiansp/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705/cache-38140d02b4384963.arrow\n",
      "Loading cached processed dataset at /Users/damiansp/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705/cache-1a30bb3aa6bcddea.arrow\n",
      "Loading cached processed dataset at /Users/damiansp/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705/cache-a9e5994305d1cb35.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['attention_mask', 'input_ids', 'label', 'text']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n",
    "emotions_encoded['train'].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8574d46c",
   "metadata": {},
   "source": [
    "## Transformers as Feature Extractors\n",
    "### Using Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5ac743d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "mod_ckpt = 'distilbert-base-uncased'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mod = AutoModel.from_pretrained(mod_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71e08a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow equivalent:\n",
    "#tf_mod = TFAutoModel.from_pretrained(mod_ckpt)\n",
    "\n",
    "# If original mod exists in only one architecture, convert:\n",
    "#tf_xlmr = TFAutoModel.from_pretrained('xlm-roberta-base', from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d59ccbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Input tensor shape: torch.Size([1, 7])'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'this is only a test'\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "f'Input tensor shape: {inputs[\"input_ids\"].size()}' \n",
    "# batch size, n tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "984b3883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[-0.1106, -0.0890,  0.0291,  ..., -0.0389,  0.1400,  0.4883],\n",
      "         [-0.2092, -0.4512, -0.0368,  ..., -0.1308,  0.3934,  0.3992],\n",
      "         [-0.2757, -0.3504,  0.2033,  ..., -0.0206, -0.0181,  0.8520],\n",
      "         ...,\n",
      "         [-0.1771, -0.2020,  0.3657,  ...,  0.0311, -0.0705,  0.8684],\n",
      "         [ 0.3047, -0.3977, -0.2155,  ..., -0.2632,  0.0557, -0.3890],\n",
      "         [ 0.9571,  0.0864, -0.4193,  ...,  0.1619, -0.6434, -0.1724]]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = mod(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d07a73b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.size() # batch, n tokens, hidden dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "072cd78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state[:, 0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37d088c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    inputs = {k: v.to(device) for k, v in batch.items()\n",
    "              if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = mod(**inputs).last_hidden_state\n",
    "    return {'hidden_state': last_hidden_state[:, 0].cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84a07171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba5c1bdb9834f48bccfa833bc8ec69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/41/jhvt_zmd3nz3x5jsqblr221r0000gn/T/ipykernel_32666/2815040384.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m emotions_encoded.set_format(\n\u001b[1;32m      2\u001b[0m     'torch', columns=['input_ids', 'attention_mask', 'label'])\n\u001b[0;32m----> 3\u001b[0;31m emotions_hidden = emotions_encoded.map(extract_hidden_states, \n\u001b[0m\u001b[1;32m      4\u001b[0m                                        batched=True)\n\u001b[1;32m      5\u001b[0m \u001b[0memotions_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mcache_file_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         return DatasetDict(\n\u001b[0;32m--> 494\u001b[0;31m             {\n\u001b[0m\u001b[1;32m    495\u001b[0m                 k: dataset.map(\n\u001b[1;32m    496\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    493\u001b[0m         return DatasetDict(\n\u001b[1;32m    494\u001b[0m             {\n\u001b[0;32m--> 495\u001b[0;31m                 k: dataset.map(\n\u001b[0m\u001b[1;32m    496\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m                     \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2091\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2092\u001b[0;31m             return self._map_single(\n\u001b[0m\u001b[1;32m   2093\u001b[0m                 \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2094\u001b[0m                 \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m         }\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/datasets/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2467\u001b[0m                         )  # Something simpler?\n\u001b[1;32m   2468\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2469\u001b[0;31m                             batch = apply_function_on_filtered_inputs(\n\u001b[0m\u001b[1;32m   2470\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   2355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2357\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2358\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m                 \u001b[0;31m# Check if the function returns updated examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/41/jhvt_zmd3nz3x5jsqblr221r0000gn/T/ipykernel_32666/3626531527.py\u001b[0m in \u001b[0;36mextract_hidden_states\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      3\u001b[0m               if k in tokenizer.model_input_names}\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mlast_hidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'hidden_state'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         return self.transformer(\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             )\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# Feed Forward Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffn_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mapply_chunking_to_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2368\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2370\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mff_chunk\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "emotions_encoded.set_format(\n",
    "    'torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "emotions_hidden = emotions_encoded.map(extract_hidden_states, \n",
    "                                       batched=True)\n",
    "emotions_hidden['train'].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46321393",
   "metadata": {},
   "source": [
    "### Create a feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a704b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = ['train', 'validation']\n",
    "X_train, X_valid = [np.array(emotions_hidden[dset]['hidden_state'])\n",
    "                    for dset in dsets]\n",
    "y_train, y_valid = [np.arry(emotions_hidden[dset]['label'])\n",
    "                    for dset in dsets]\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a8ef3c",
   "metadata": {},
   "source": [
    "### Visualizing the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c85d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = MinMaxScaler().fit_transform(X_train)\n",
    "mapper = UMAP(n_components=2, metric='cosine').fit(X_scaled) # 2D proj\n",
    "df_emb = pd.DataFrame(mapper.embedding_, columns=['X', 'Y'])\n",
    "df_emb['label'] = y_train\n",
    "df_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350d7bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(7, 5))\n",
    "axes = axes.flatten()\n",
    "cmaps = ['Greys', 'Blues', 'Oranges', 'Reds', 'Purples', 'Greens']\n",
    "labels = emotions['train'].features['label'].names\n",
    "\n",
    "for i, (label, cmap) in enumerate(zip(labels, cmaps)):\n",
    "    sub = df_emb[df_emb.label == i]\n",
    "    axes[i].hexbin(\n",
    "        sub.X, sub.Y, cmap=cmap, gridsize=20, linewidths=(0,))\n",
    "    axes[i].set_title(label)\n",
    "    axis[i].set_xticks([]), axes[i].set_yticks([])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97cf7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9b76930",
   "metadata": {},
   "source": [
    "### Train a Simple Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521ee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "log_reg.score(X_valid, y_valid) # accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63333dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But recall, this is an unbalanced multi-class set\n",
    "for strategy in ['most_frequent', 'uniform']:\n",
    "    dummy = DummyClassifier(strategy=strategy)\n",
    "    dummy.fit(X_train, y_train)\n",
    "    score = dummy.score(X_valid, y_valid)\n",
    "    print(f'{strategy}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06299473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(actual, preds, labels):\n",
    "    cm = confusion_matrix(actual, preds, normalize='true')\n",
    "    fix, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                  display_labels=labels)\n",
    "    disp.plot(cmap='Blues', values_format='.2f', ax=ax, colorbar=False)\n",
    "    plt.title('Normalized Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9833e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = log_reg.predict(X_valid)\n",
    "plot_confusion_matrix(y_valid, preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e94c1c6",
   "metadata": {},
   "source": [
    "## Fine Tuning Transformers\n",
    "### Loading a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35039465",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = 6\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(model_ckpt, num_labels=n_labels)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e161c8",
   "metadata": {},
   "source": [
    "### Define Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.prdictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    return {'accuracy': acc, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c9202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995170ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
