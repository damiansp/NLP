{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# NLP with PyTorch\r\n",
      "\r\n",
      "Learning from:\r\n",
      "\r\n",
      "D. Rao & B. McMahan (2019) _Natural Language Processing with PyTorch_ (O'Reilly)\r\n",
      "\r\n",
      "<a href=\"https://github.com/joosthub/PyTorchNLPBook\">Book Repo</a>.\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   argparse import Namespace\n",
    "from   collections import Counter\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from   torch.optim import Adam\n",
    "from   torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    '''Class to process text and extract Vocabulary for mapping'''\n",
    "    \n",
    "    def __init__(\n",
    "            self, token_to_idx=None, add_unk=True, unk_token='<UNK>'):\n",
    "        '''\n",
    "        Args:\n",
    "          token_to_idx (dict): a pre-existing map of tokens to indices \n",
    "          add_unk (bool): flag indicating whether to add the UNK token\n",
    "          unk_token (str): the UNK token to add to the Vocabulary\n",
    "        '''\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {\n",
    "            idx: token for token, idx in self._token_to_idx.items()}\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "            \n",
    "    def to_serializable(self):\n",
    "        '''Returns a dict that can be serialized'''\n",
    "        return {'token_to_idx': self._token_to_idx,\n",
    "                'add_unk': self._add_unk,\n",
    "                'unk_token': self._unk_token}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        '''Instantiates the Vocabulary from a serialized dict'''\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        '''\n",
    "        Update mapping dicts base on the token\n",
    "        Args:\n",
    "          token (str): the item to add to the Vocabulary\n",
    "        Returns:\n",
    "          index (int): the int corresponding to the token\n",
    "        '''\n",
    "        try:\n",
    "            index = self._token_to_idx[token]\n",
    "        except KeyError:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        '''\n",
    "        Add a list of tokens int the Vocabulary\n",
    "        Args:\n",
    "          tokens (list<str>)\n",
    "        Returns:\n",
    "          indices (list<int>): indices for input tokens\n",
    "        '''\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        '''\n",
    "        Retrieve the index associated with the token or the UNK index if\n",
    "        token not present\n",
    "        Args:\n",
    "          token (str): the token to look up\n",
    "        Returns:\n",
    "          index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "          <unk_index> must be >= 0 (having been added into the Vocabulary)\n",
    "          for UNK functionality\n",
    "        '''\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        '''\n",
    "        Return the token associated with the index\n",
    "        Args:\n",
    "          index (int): the index to look up\n",
    "        Returns:\n",
    "          token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "          KeyError: if index not in Vocabulary\n",
    "        '''\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(f'the index {index} is not in the Vocabulary')\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'<Vocabulary(size={len(self)})>'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameVectorizer:\n",
    "    '''\n",
    "    The Vectorizer which coordinates the Vocabularies and puts them to use\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, surname_vocab, nationality_vocab):\n",
    "        '''\n",
    "        Args:\n",
    "          surname_vocab (Vocabulary): maps characters to integers\n",
    "          nationality_vocab (Vocabulary): maps nationalities to integers\n",
    "        '''\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "        \n",
    "    def vectorize(self, surname):\n",
    "        '''\n",
    "        Vectorize the provided surname\n",
    "        Args:\n",
    "          surname (str): surname\n",
    "        Returns:\n",
    "          one_hot (np.ndarray): a collapsed one-hot encoding\n",
    "        '''\n",
    "        vocab = self.surname_vocab\n",
    "        one_hot = np.zeros(len(vocab), dtype=np.float32)\n",
    "        for token in surname:\n",
    "            one_hot[vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        '''\n",
    "        Instantiate the vectorizer from the data set DataFrame\n",
    "        Args:\n",
    "          surname_df (pandas.DataFrame): surnames data set\n",
    "        Returns:\n",
    "          instance of the SurnameVectorizer\n",
    "        '''\n",
    "        surname_vocab = Vocabulary(unk_token='@')\n",
    "        nationality_vocab = Vocabulary(add_unk=False)\n",
    "        for index, row in surname_df.iterrows():\n",
    "            for letter in row.surname:\n",
    "                surname_vocab.add_token(letter)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "        return cls(surname_vocab, nationality_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        surname_vocab = Vocabulary.from_serializable(\n",
    "            contents['surname_vocab'])\n",
    "        nationality_vocab = Vocabulary.from_serializable(\n",
    "            contents['nationality_vocab'])\n",
    "        return cls(surname_vocab, nationality_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {\n",
    "            'surname_vocab': self.surname_vocab.to_serializable(),\n",
    "            'nationality_vocab': self.nationality_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HERE ###\n",
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "        '''\n",
    "        Args:\n",
    "          surname_df (pandas.DataFrame): the data\n",
    "          vectorizer (SurnameVectorizer): vectorizer instantiated from \n",
    "            data set\n",
    "        '''\n",
    "        self.surname_df = surname_df\n",
    "        self._vectorizer = vectorizer\n",
    "        self.train_df = self.surname_df[self.surname_df.split == 'train']\n",
    "        self.val_df   = self.surname_df[self.surname_df.split == 'val']\n",
    "        self.test_df  = self.surname_df[self.surname_df.split == 'test']\n",
    "        self.train_size      = len(self.train_df)\n",
    "        self.validation_size = len(self.val_df)\n",
    "        self.test_size       = len(self.test_df)\n",
    "        self._lookup_dict = {\n",
    "            'train': (self.train_df, self.train_size),\n",
    "            'val':   (self.val_df,   self.validation_size),\n",
    "            'test':  (self.test_df,  self.test_size)}\n",
    "        self.set_split('train')\n",
    "        # Class weights\n",
    "        class_counts = surname_df.nationality.value_counts().to_dict()\n",
    "        \n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.nationality_vocab.lookup_token(item[0])\n",
    "        \n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1. / torch.tensor(frequencies, \n",
    "                                               dtype=torch.float32)\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
    "        '''\n",
    "        Load data set and make a new vectorizer form scratch\n",
    "        Args:\n",
    "          surname_csv (str): location of data set\n",
    "        Returns:\n",
    "          an instance of SurnameDataset\n",
    "        '''\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        train_surname_df = surname_df[surname_df.split == 'train']\n",
    "        return cls(surname_df, \n",
    "                   SurnameVectorizer.from_dataframe(train_surname_df))\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(\n",
    "            cls, surname_csv, vectorizer_filepath):\n",
    "        '''\n",
    "        Load data set and the corresponding vectorizer.  Used in the case\n",
    "        that vectorizer has been cached for re-use\n",
    "        Args:\n",
    "          surname_csv (str): location of data set\n",
    "          vectorizer_filepath (str): location of saved vectorizer\n",
    "        Returns:\n",
    "          an instance of SurnameDataset\n",
    "        '''\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(surname_df, vectorizer)\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        '''\n",
    "        Static method for loading the vectorizer from file\n",
    "        Args:\n",
    "          vectorizer_filepath (str): location of the serialized vectorizer\n",
    "        Returns:\n",
    "          an instance of SurnameVectorizer\n",
    "        '''\n",
    "        with open(vectorizer_file_path) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "        \n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        '''Saves the vectorizer to disk using json.\n",
    "        Args:\n",
    "          vectorizer_filepath (str): location to save vectorizer\n",
    "        '''\n",
    "        with open(vectorizer_filepath, 'w') as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "            \n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_split(self, split='train'):\n",
    "        '''Selects the splits in the data set using a column in the DF'''\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        row = self._target_df.iloc[idx]\n",
    "        surname_vector = self._vectorizer.vectorize(row.surname)\n",
    "        nationality_idx = self._vectorizer.nationality_vocab.lookup_token(\n",
    "            row.nationality)\n",
    "        return {'x_surname': surname_vector, \n",
    "                'y_nationality': nationality_idx}\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        '''Given a batch size, return number of batches in data set'''\n",
    "        return len(self) // batch_size    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape: torch.Size([2, 3])\n",
      "Values:\n",
      "tensor([[2.4515e-01, 8.9136e-01, 3.9119e-04],\n",
      "        [6.7195e-01, 5.7999e-01, 1.0281e-01]])\n"
     ]
    }
   ],
   "source": [
    "x_input = torch.rand(BATCH, INPUT)\n",
    "describe(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape: torch.Size([2, 4])\n",
      "Values:\n",
      "tensor([[-0.0720,  0.1462, -0.3504, -0.0369],\n",
      "        [-0.1236,  0.1536, -0.3663,  0.1226]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "y_output = mlp(x_input, apply_softmax=False)\n",
    "describe(y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape: torch.Size([2, 4])\n",
      "Values:\n",
      "tensor([[0.2477, 0.3081, 0.1875, 0.2566],\n",
      "        [0.2281, 0.3010, 0.1790, 0.2918]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "y_output = mlp(x_input, apply_softmax=True)\n",
    "describe(y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_output.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(\n",
    "        dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
    "    '''\n",
    "    Generator function that wraps the PyTorch DataLoader. Ensures each \n",
    "    tensor is on the write device location\n",
    "    '''\n",
    "    dataloader = DataLoader(dataset=dataset, \n",
    "                            batchsize=batch_size, \n",
    "                            shuffle=shuffle, \n",
    "                            drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameClassifier(nn.Module):\n",
    "    '''A 2-layer multilayer perceptron for classifying surnames'''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        '''\n",
    "        Args:\n",
    "          input_dim (int): size of input vectors\n",
    "          hiddden_dim (int): output size of first Linear layer\n",
    "          output_dim (int): output size of second Linear layer\n",
    "        '''\n",
    "        super(SurnameClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        '''\n",
    "        The forward pass\n",
    "        Args:\n",
    "          x_in (torch.Tensor): an input data tensor (batch, input_dim)\n",
    "          apply_softmax (bool): should be False if used with the cross-\n",
    "            entropy losses\n",
    "        Returns:\n",
    "          the resulting tensor (batch, output_dim)\n",
    "        '''\n",
    "        intermediate_vector = F.relu(self.fc1(x_in))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "        return prediction_vector\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../../data/surnames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surnames.csv              surnames_with_splits.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls ../../data/surnames/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat ../../data/surnames/surnames.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Data and path info\n",
    "    surname_csv = f'{DATA_DIR}/surnames_with_splits.csv',\n",
    "    vectorizer_file = 'vectorizer.json',\n",
    "    model_state_file = 'model.pth',\n",
    "    save_dir = 'model_storage/surname_mlp',\n",
    "    \n",
    "    # Model hyperparams\n",
    "    seed = 12345,\n",
    "    epochs = 100,\n",
    "    early_stopping_criteria = 5,\n",
    "    learning_rate = 0.001,\n",
    "    batch = 64\n",
    "    \n",
    "    # Additional runtime options...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SurnameDataset.load_dataset_and_make_vectorizer(\n",
    "    args.surname_csv)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "classifier = SurnameClassifier(\n",
    "    input_dim=len(vectorizer.surname_vocab),\n",
    "    hidden_dim=HIDDEN,\n",
    "    output_dim=len(vectorizer.nationality_vocab))\n",
    "#classifier = classifier.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = Adam(classifier.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-811cb6f1f0fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_surname'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_nationality'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss_batch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrunning_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_dict' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "y_pred = classifier(batch_dict['x_surname'])\n",
    "loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "loss_batch = lass.to('cpu').item()\n",
    "running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
